
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlfOoTgcg0Uy"
      },
      "source": [
        "# Pneumonia Detection Model Training\n",
        "\n",
        "This notebook trains a CNN model to detect pneumonia from chest X-ray images using the Kaggle chest X-ray pneumonia dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm1ePeJqg7kB"
      },
      "source": [
        "## Setup and Dataset Download\n",
        "\n",
        "First, we'll install necessary libraries and download the dataset from Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dftWnK4lhANx"
      },
      "source": [
        "# Install required packages\n",
        "!pip install kaggle tensorflow matplotlib numpy pandas scikit-learn"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UHXhGg8hIp5"
      },
      "source": [
        "### Configure Kaggle API\n",
        "\n",
        "To download the dataset, you need to provide your Kaggle API credentials. Follow these steps:\n",
        "1. Go to your Kaggle account settings (https://www.kaggle.com/me/account)\n",
        "2. Scroll down to 'API' section and click 'Create New API Token'\n",
        "3. This will download a 'kaggle.json' file\n",
        "4. Upload this file in the next cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nv8uKw2chNz9"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() # Upload your kaggle.json file here"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKr7d5H4hQvi"
      },
      "source": [
        "# Configure Kaggle API\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6LyC_t8hUP4"
      },
      "source": [
        "# Download the chest X-ray dataset\n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
        "!unzip -q chest-xray-pneumonia.zip"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM_yQnO4hYGQ"
      },
      "source": [
        "## Data Exploration and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoLJdYV0hbp4"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F0N1hFthfSO"
      },
      "source": [
        "# Define dataset paths\n",
        "train_dir = 'chest_xray/train'\n",
        "val_dir = 'chest_xray/val'\n",
        "test_dir = 'chest_xray/test'\n",
        "\n",
        "# Check class distribution\n",
        "def count_images(directory):\n",
        "    normal = len(os.listdir(os.path.join(directory, 'NORMAL')))\n",
        "    pneumonia = len(os.listdir(os.path.join(directory, 'PNEUMONIA')))\n",
        "    return normal, pneumonia\n",
        "\n",
        "train_normal, train_pneumonia = count_images(train_dir)\n",
        "val_normal, val_pneumonia = count_images(val_dir)\n",
        "test_normal, test_pneumonia = count_images(test_dir)\n",
        "\n",
        "print(f\"Training set: Normal={train_normal}, Pneumonia={train_pneumonia}\")\n",
        "print(f\"Validation set: Normal={val_normal}, Pneumonia={val_pneumonia}\")\n",
        "print(f\"Test set: Normal={test_normal}, Pneumonia={test_pneumonia}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBmSniIYhjHf"
      },
      "source": [
        "# Check if validation set is too small (it's known to be small in this dataset)\n",
        "# If it's too small, we'll create a new validation set from the training data\n",
        "if val_normal < 100 or val_pneumonia < 100:\n",
        "    print(\"Validation set is too small. Will create a new validation set from training data.\")\n",
        "    \n",
        "    # We'll move 20% of training data to validation\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from shutil import copyfile\n",
        "    import random\n",
        "    \n",
        "    # Create new val directory if it doesn't exist\n",
        "    os.makedirs('chest_xray/new_val/NORMAL', exist_ok=True)\n",
        "    os.makedirs('chest_xray/new_val/PNEUMONIA', exist_ok=True)\n",
        "    \n",
        "    # Normal images\n",
        "    normal_images = os.listdir(os.path.join(train_dir, 'NORMAL'))\n",
        "    normal_train, normal_val = train_test_split(normal_images, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Pneumonia images\n",
        "    pneumonia_images = os.listdir(os.path.join(train_dir, 'PNEUMONIA'))\n",
        "    pneumonia_train, pneumonia_val = train_test_split(pneumonia_images, test_size=0.2, random_state=42)\n",
        "    \n",
        "    # Copy validation images\n",
        "    for img in normal_val:\n",
        "        copyfile(os.path.join(train_dir, 'NORMAL', img), \n",
        "                os.path.join('chest_xray/new_val/NORMAL', img))\n",
        "        \n",
        "    for img in pneumonia_val:\n",
        "        copyfile(os.path.join(train_dir, 'PNEUMONIA', img), \n",
        "                os.path.join('chest_xray/new_val/PNEUMONIA', img))\n",
        "    \n",
        "    val_dir = 'chest_xray/new_val'\n",
        "    val_normal, val_pneumonia = count_images(val_dir)\n",
        "    print(f\"New validation set: Normal={val_normal}, Pneumonia={val_pneumonia}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6-aS-SvhoYH"
      },
      "source": [
        "# Display sample images\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Normal samples\n",
        "for i in range(3):\n",
        "    plt.subplot(2, 3, i+1)\n",
        "    img_path = os.path.join(train_dir, 'NORMAL', os.listdir(os.path.join(train_dir, 'NORMAL'))[i])\n",
        "    img = plt.imread(img_path)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title('Normal')\n",
        "    plt.axis('off')\n",
        "\n",
        "# Pneumonia samples\n",
        "for i in range(3):\n",
        "    plt.subplot(2, 3, i+4)\n",
        "    img_path = os.path.join(train_dir, 'PNEUMONIA', os.listdir(os.path.join(train_dir, 'PNEUMONIA'))[i])\n",
        "    img = plt.imread(img_path)\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.title('Pneumonia')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDU4KCsKhtAO"
      },
      "source": [
        "## Data Preprocessing and Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTKtYh6thwIH"
      },
      "source": [
        "# Define image dimensions and batch size\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Data augmentation for training set\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Only rescaling for validation and test sets\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load the datasets\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZzRPH4h051"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQZB5jNsh3tP"
      },
      "source": [
        "def create_model():\n",
        "    model = Sequential([\n",
        "        # First convolutional block\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n",
        "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "        \n",
        "        # Second convolutional block\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "        \n",
        "        # Third convolutional block\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.25),\n",
        "        \n",
        "        # Fully connected layers\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    \n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model()\n",
        "\n",
        "# Display model summary\n",
        "model.summary()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhW7yFcxh7fG"
      },
      "source": [
        "## Model Training with Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKL_eA4nh-bO"
      },
      "source": [
        "# Define callbacks\n",
        "callbacks = [\n",
        "    # Early stopping to prevent overfitting\n",
        "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    \n",
        "    # Save the best model\n",
        "    ModelCheckpoint('pneumonia_model.h5', save_best_only=True, monitor='val_loss'),\n",
        "    \n",
        "    # Reduce learning rate when a metric has stopped improving\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "# Calculate class weights to handle imbalance\n",
        "total_train = train_normal + train_pneumonia\n",
        "weight_normal = total_train / (2 * train_normal)\n",
        "weight_pneumonia = total_train / (2 * train_pneumonia)\n",
        "class_weights = {0: weight_normal, 1: weight_pneumonia}\n",
        "\n",
        "print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "# Number of training and validation steps\n",
        "STEPS_PER_EPOCH = train_generator.samples // BATCH_SIZE\n",
        "VALIDATION_STEPS = validation_generator.samples // BATCH_SIZE + 1\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=VALIDATION_STEPS,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights\n",
        ")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGrpkXRdiCWl"
      },
      "source": [
        "## Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a9bNT3aiFdA"
      },
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HV0Xb-OiInO"
      },
      "source": [
        "# Load the best model\n",
        "best_model = tf.keras.models.load_model('pneumonia_model.h5')\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = best_model.evaluate(test_generator, steps=test_generator.samples // BATCH_SIZE + 1)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r3U7SiXiMAm"
      },
      "source": [
        "# Predict test set\n",
        "test_generator.reset()\n",
        "predictions = best_model.predict(test_generator, steps=test_generator.samples // BATCH_SIZE + 1)\n",
        "pred_classes = (predictions > 0.5).astype(\"int32\")\n",
        "\n",
        "# Get true labels\n",
        "true_classes = test_generator.classes\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(true_classes, pred_classes)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(2)\n",
        "plt.xticks(tick_marks, ['Normal', 'Pneumonia'], rotation=45)\n",
        "plt.yticks(tick_marks, ['Normal', 'Pneumonia'])\n",
        "\n",
        "# Display the actual count values\n",
        "threshold = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > threshold else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "target_names = ['Normal', 'Pneumonia']\n",
        "print(classification_report(true_classes, pred_classes, target_names=target_names))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz0xsX7ZiPfP"
      },
      "source": [
        "## Convert Model to TensorFlow.js Format for Web Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5B-ULESiSVf"
      },
      "source": [
        "# Install tensorflowjs\n",
        "!pip install tensorflowjs"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCVY1aVriU5m"
      },
      "source": [
        "# Create output directory\n",
        "!mkdir -p pneumonia_model_tfjs\n",
        "\n",
        "# Convert model to TensorFlow.js format\n",
        "import tensorflowjs as tfjs\n",
        "tfjs.converters.save_keras_model(best_model, 'pneumonia_model_tfjs')"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWPiDxeQiX7m"
      },
      "source": [
        "# Zip the model files to download\n",
        "!zip -r pneumonia_model_tfjs.zip pneumonia_model_tfjs"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_uCG5m5iatn"
      },
      "source": [
        "# Download the converted model\n",
        "from google.colab import files\n",
        "files.download('pneumonia_model_tfjs.zip')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN03cLBgidUO"
      },
      "source": [
        "## Test Model Prediction on a Sample Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDkInBf1ifuW"
      },
      "source": [
        "# Test prediction on a sample image\n",
        "import random\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Choose a random test image\n",
        "pneumonia_test_dir = os.path.join(test_dir, 'PNEUMONIA')\n",
        "normal_test_dir = os.path.join(test_dir, 'NORMAL')\n",
        "\n",
        "# Randomly pick normal or pneumonia\n",
        "if random.choice([True, False]):\n",
        "    img_path = os.path.join(pneumonia_test_dir, random.choice(os.listdir(pneumonia_test_dir)))\n",
        "    true_class = 'Pneumonia'\n",
        "else:\n",
        "    img_path = os.path.join(normal_test_dir, random.choice(os.listdir(normal_test_dir)))\n",
        "    true_class = 'Normal'\n",
        "\n",
        "# Load and preprocess the image\n",
        "img = image.load_img(img_path, target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
        "img_array = image.img_to_array(img)\n",
        "img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "# Make prediction\n",
        "prediction = best_model.predict(img_array)\n",
        "predicted_class = 'Pneumonia' if prediction[0][0] > 0.5 else 'Normal'\n",
        "confidence = prediction[0][0] if predicted_class == 'Pneumonia' else 1 - prediction[0][0]\n",
        "\n",
        "# Display the image and prediction\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(img)\n",
        "plt.title(f\"True: {true_class}, Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"True label: {true_class}\")\n",
        "print(f\"Predicted: {predicted_class} with {confidence:.2%} confidence\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQxJJxX3ii8G"
      },
      "source": [
        "## Next Steps for Web Integration\n",
        "\n",
        "Now that you have trained the model and converted it to TensorFlow.js format, you need to:\n",
        "\n",
        "1. Download the `pneumonia_model_tfjs.zip` file using the download link above\n",
        "2. Extract it to your web application's public directory (e.g., `/public/model/`)\n",
        "3. Update the `src/lib/tensorflow-model.ts` file in your React application to load this model instead of the placeholder\n",
        "\n",
        "```typescript\n",
        "// Replace this line:\n",
        "// this.model = await this.createDummyModel();\n",
        "\n",
        "// With:\n",
        "this.model = await tf.loadLayersModel('/model/model.json');\n",
        "```\n",
        "\n",
        "4. Update the prediction logic to use the actual model prediction rather than the placeholder logic\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
